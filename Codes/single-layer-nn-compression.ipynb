{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fe2c47f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T21:55:27.150102Z",
     "iopub.status.busy": "2025-11-09T21:55:27.149599Z",
     "iopub.status.idle": "2025-11-09T21:55:46.450724Z",
     "shell.execute_reply": "2025-11-09T21:55:46.449874Z"
    },
    "papermill": {
     "duration": 19.30793,
     "end_time": "2025-11-09T21:55:46.452309",
     "exception": false,
     "start_time": "2025-11-09T21:55:27.144379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as mcolors\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import traceback\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a752d365",
   "metadata": {
    "_cell_guid": "844235e8-8b62-4386-ad62-e6201bb26f92",
    "_uuid": "26b1bf8d-a2bc-4937-a1ad-cf8d5229ed42",
    "execution": {
     "iopub.execute_input": "2025-11-09T21:55:46.461088Z",
     "iopub.status.busy": "2025-11-09T21:55:46.460255Z",
     "iopub.status.idle": "2025-11-09T21:55:46.465745Z",
     "shell.execute_reply": "2025-11-09T21:55:46.465115Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.010838,
     "end_time": "2025-11-09T21:55:46.466973",
     "exception": false,
     "start_time": "2025-11-09T21:55:46.456135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SingleNN(nn.Module):\n",
    "    def __init__(self, input_dim = 3, hidden_dim = 10, num_layer = 1, output_dim = 1, dropout = 0.1):\n",
    "        super(SingleNN, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(input_dim, hidden_dim),\\\n",
    "                                 nn.SiLU(),\\\n",
    "                                 nn.Linear(hidden_dim,15),\\\n",
    "                                 nn.SiLU(),\\\n",
    "                                 nn.Linear(15, output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "criterion  = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3cf202",
   "metadata": {
    "_cell_guid": "d18dffbe-d3fe-4c3a-8c91-f912625e72ab",
    "_uuid": "1d4c3449-9389-4e9e-9b5b-4bc66330b225",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-09T21:55:46.474492Z",
     "iopub.status.busy": "2025-11-09T21:55:46.474261Z",
     "iopub.status.idle": "2025-11-09T21:55:46.595486Z",
     "shell.execute_reply": "2025-11-09T21:55:46.594516Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.126491,
     "end_time": "2025-11-09T21:55:46.596735",
     "exception": false,
     "start_time": "2025-11-09T21:55:46.470244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples : 4000 | Validation Samples: 1000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/single-layer-models/F1_data.csv\", header = None)\n",
    "df = df.drop(index = 0)\n",
    "df = df.astype(float)\n",
    "\n",
    "y = torch.tensor(df.iloc[0:, 4].values, dtype = torch.float).unsqueeze(1)\n",
    "X = torch.tensor(df.iloc[:,0:4].values, dtype = torch.float)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Standardize inputs using training statistics\n",
    "X_mean, X_std = X_train.mean(dim=0, keepdim=True), X_train.std(dim=0, keepdim=True)\n",
    "X_train_std = (X_train - X_mean) / X_std\n",
    "X_val_std = (X_val - X_mean) / X_std\n",
    "\n",
    "# Standardize outputs (helps numerical stability)\n",
    "y_mean, y_std = y_train.mean(), y_train.std()\n",
    "y_train_std = (y_train - y_mean) / y_std\n",
    "y_val_std = (y_val - y_mean) / y_std\n",
    "\n",
    "\n",
    "train_ds = TensorDataset(X_train_std, y_train_std)\n",
    "val_ds = TensorDataset(X_val_std, y_val_std)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_ds, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(val_ds, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "print(f\"Training samples : {len(train_ds)} | Validation Samples: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf530d3",
   "metadata": {
    "_cell_guid": "abfb5f8f-d0fa-41c0-801c-b145b718abfe",
    "_uuid": "45e70cb6-68ee-4c95-a19e-dade07984126",
    "execution": {
     "iopub.execute_input": "2025-11-09T21:55:46.606436Z",
     "iopub.status.busy": "2025-11-09T21:55:46.606221Z",
     "iopub.status.idle": "2025-11-09T21:55:46.687732Z",
     "shell.execute_reply": "2025-11-09T21:55:46.687126Z"
    },
    "papermill": {
     "duration": 0.08849,
     "end_time": "2025-11-09T21:55:46.689014",
     "exception": false,
     "start_time": "2025-11-09T21:55:46.600524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkCompressor:\n",
    "    def __init__(self, model, compression_ratio=0.5, Tmin=0.01, alpha=0.99, \n",
    "                 PERTURB=0.0005, STOP=1e-5, T=80, seed=42, device='auto'):\n",
    "        \"\"\"\n",
    "        Initialize the neural network compressor using deterministic annealing.\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch neural network model\n",
    "            compression_ratio: Target compression ratio for hidden layers\n",
    "            Tmin: Minimum temperature for annealing\n",
    "            alpha: Annealing rate\n",
    "            PERTURB: Perturbation factor for centroids\n",
    "            STOP: Convergence threshold\n",
    "            T: Initial temperature\n",
    "            seed: Random seed for reproducibility\n",
    "            device: Device to use ('cuda', 'cpu', or 'auto')\n",
    "        \"\"\"\n",
    "        self.model = deepcopy(model)\n",
    "        self.compression_ratio = compression_ratio\n",
    "        self.Tmin = Tmin\n",
    "        self.alpha = alpha\n",
    "        self.PERTURB = PERTURB\n",
    "        self.STOP = STOP\n",
    "        self.T = T\n",
    "        self.seed = seed\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "        \n",
    "        # Set device\n",
    "        if device == 'auto':\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Try to move model to device with error handling\n",
    "        try:\n",
    "            self.model.to(self.device)\n",
    "            print(\"Model successfully moved to device.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error moving model to device: {e}\")\n",
    "            print(\"Falling back to CPU.\")\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            self.model.to(self.device)\n",
    "        \n",
    "        # Store original model parameters for comparison\n",
    "        self.original_params = {name: param.data.clone() for name, param in self.model.named_parameters()}\n",
    "        self.compressed_params = {}\n",
    "        self.compression_stats = {}\n",
    "        self.association_matrices = {}\n",
    "        self.compressed_model = None\n",
    "        \n",
    "    def calculate_distortion(self, X, Y):\n",
    "        \"\"\"\n",
    "        Calculate the squared Euclidean distance between each point in X and each centroid in Y.\n",
    "        Uses GPU acceleration when available.\n",
    "        \n",
    "        Args:\n",
    "            X (torch.Tensor): Data matrix of shape (M, N)\n",
    "            Y (torch.Tensor): Centroid matrix of shape (K, N)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Distance matrix of shape (M, K)\n",
    "        \"\"\"\n",
    "        X_sum_sq = torch.sum(X**2, dim=1, keepdim=True)\n",
    "        Y_sum_sq = torch.sum(Y**2, dim=1, keepdim=True).T\n",
    "        D = X_sum_sq + Y_sum_sq - 2 * X @ Y.T\n",
    "        return D\n",
    "    \n",
    "    def run_deterministic_annealing(self, X, K):\n",
    "        \"\"\"\n",
    "        Run deterministic annealing clustering algorithm on GPU.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Data matrix of shape (M, N)\n",
    "            K (int): Number of clusters\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (centroids Y, association matrix P)\n",
    "        \"\"\"\n",
    "        M, N = X.shape\n",
    "        \n",
    "        # Convert to PyTorch tensor and move to device\n",
    "        try:\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating tensor on device: {e}\")\n",
    "            print(\"Falling back to CPU for this operation.\")\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32, device='cpu')\n",
    "        \n",
    "        # Px is the weight for each data point, assuming uniform weights\n",
    "        Px = torch.full((M, 1), 1 / M, device=X_tensor.device)\n",
    "        \n",
    "        # Initialize centroids to the weighted mean of data\n",
    "        initial_mean = (Px.T @ X_tensor).reshape(1, -1)\n",
    "        Y = initial_mean.repeat(K, 1)\n",
    "        \n",
    "        T = self.T\n",
    "        while T >= self.Tmin:\n",
    "            L_old = float('inf')\n",
    "            while True:\n",
    "                # Calculate distortion matrix\n",
    "                D = self.calculate_distortion(X_tensor, Y)\n",
    "                \n",
    "                # Calculate probability matrix using softmax\n",
    "                D_bar = D - torch.min(D, dim=1, keepdim=True).values\n",
    "                num = torch.exp(-D_bar / T)\n",
    "                den = torch.sum(num, dim=1, keepdim=True)\n",
    "                P = num / den\n",
    "                \n",
    "                # Update centroids\n",
    "                Py = P.T @ Px\n",
    "                Py[Py == 0] = 1e-10  # Avoid division by zero\n",
    "                Y = (P.T @ (X_tensor * Px)) / Py.reshape(-1, 1)\n",
    "                \n",
    "                # Add small random perturbation\n",
    "                Y += self.PERTURB * torch.rand(*Y.shape, device=Y.device)\n",
    "                \n",
    "                # Calculate loss function\n",
    "                L = -T * (Px.T @ torch.log(torch.sum(torch.exp(-D_bar / T), dim=1, keepdim=True)))\n",
    "                \n",
    "                # Check for convergence\n",
    "                if torch.abs(L - L_old) < self.STOP:\n",
    "                    break\n",
    "                    \n",
    "                L_old = L\n",
    "                \n",
    "            # Decrease temperature\n",
    "            T *= self.alpha\n",
    "            \n",
    "        # Convert results back to numpy for compatibility with PyTorch layers\n",
    "        return Y.cpu().numpy(), P.cpu().numpy()\n",
    "    \n",
    "    def compress_layer(self, layer, input_dim=None):\n",
    "        \"\"\"\n",
    "        Compress a single layer using deterministic annealing.\n",
    "        \n",
    "        Args:\n",
    "            layer: PyTorch layer (Linear or Conv2d)\n",
    "            input_dim: Input dimension for the layer (if needed)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (compressed_layer, association_matrix)\n",
    "        \"\"\"\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            return self.compress_linear_layer(layer)\n",
    "        elif isinstance(layer, nn.Conv2d):\n",
    "            return self.compress_conv_layer(layer)\n",
    "        else:\n",
    "            # For non-compressible layers, return the original layer and identity matrix\n",
    "            return layer, None\n",
    "    \n",
    "    def compress_linear_layer(self, layer):\n",
    "        \"\"\"\n",
    "        Compress a linear layer using deterministic annealing, treating bias as a special neuron.\n",
    "        \"\"\"\n",
    "        # Get weight matrix and bias\n",
    "        weight_matrix = layer.weight.data.cpu().numpy()  # (out_features, in_features)\n",
    "        out_features, in_features = weight_matrix.shape\n",
    "\n",
    "        print(weight_matrix.shape)\n",
    "        \n",
    "        # Create augmented weight matrix including bias as a special neuron\n",
    "        if layer.bias is not None:\n",
    "            bias = layer.bias.data.cpu().numpy()  # (out_features,)\n",
    "            # Add a column for the bias neuron (which always outputs 1)\n",
    "            augmented_matrix = np.hstack([weight_matrix, bias.reshape(-1, 1)])\n",
    "        else:\n",
    "            # If no bias, just use the weight matrix\n",
    "            augmented_matrix = weight_matrix\n",
    "        \n",
    "        # Determine number of clusters based on compression ratio\n",
    "        K = max(1, int(out_features * self.compression_ratio))\n",
    "        \n",
    "        # Skip compression if K equals out_features (compression_ratio=1)\n",
    "        if K >= out_features:\n",
    "            return layer, None\n",
    "        \n",
    "        # Run deterministic annealing on the augmented matrix\n",
    "        centroids, P = self.run_deterministic_annealing(augmented_matrix, K)\n",
    "        \n",
    "        # Get the device and dtype from the original layer\n",
    "        device = layer.weight.device\n",
    "        dtype = layer.weight.dtype\n",
    "        \n",
    "        # Split centroids into weights and bias\n",
    "        if layer.bias is not None:\n",
    "            # Last column of centroids represents the bias\n",
    "            compressed_weights = centroids[:, :-1]\n",
    "            compressed_bias = centroids[:, -1]\n",
    "        else:\n",
    "            compressed_weights = centroids\n",
    "            compressed_bias = None\n",
    "        \n",
    "        # Create compressed linear layer\n",
    "        compressed_layer = nn.Linear(in_features, K).to(device)\n",
    "        compressed_layer.weight.data = torch.tensor(compressed_weights, device=device, dtype=dtype)\n",
    "        \n",
    "        if compressed_bias is not None:\n",
    "            compressed_layer.bias.data = torch.tensor(compressed_bias, device=device, dtype=dtype)\n",
    "        else:\n",
    "            compressed_layer.bias = None\n",
    "            \n",
    "        return compressed_layer, P\n",
    "        \n",
    "    def compress_conv_layer(self, layer):\n",
    "        \"\"\"\n",
    "        Compress a convolutional layer using deterministic annealing, treating bias as a special neuron.\n",
    "        \"\"\"\n",
    "        # Get weight tensor and bias\n",
    "        weight_tensor = layer.weight.data.cpu().numpy()  # (out_channels, in_channels, kernel_h, kernel_w)\n",
    "        out_channels, in_channels, kernel_h, kernel_w = weight_tensor.shape\n",
    "        \n",
    "        # Reshape weight tensor to 2D matrix\n",
    "        weight_matrix = weight_tensor.reshape(out_channels, -1)  # (out_channels, in_channels * kernel_h * kernel_w)\n",
    "        \n",
    "        # Create augmented weight matrix including bias as a special neuron\n",
    "        if layer.bias is not None:\n",
    "            bias = layer.bias.data.cpu().numpy()  # (out_channels,)\n",
    "            # Add a column for the bias neuron (which always outputs 1)\n",
    "            augmented_matrix = np.hstack([weight_matrix, bias.reshape(-1, 1)])\n",
    "        else:\n",
    "            # If no bias, just use the weight matrix\n",
    "            augmented_matrix = weight_matrix\n",
    "        \n",
    "        # Determine number of clusters based on compression ratio\n",
    "        K = max(1, int(out_channels * self.compression_ratio))\n",
    "        \n",
    "        # Skip compression if K equals out_channels (compression_ratio=1)\n",
    "        if K >= out_channels:\n",
    "            return layer, None\n",
    "        \n",
    "        # Run deterministic annealing on the augmented matrix\n",
    "        centroids, P = self.run_deterministic_annealing(augmented_matrix, K)\n",
    "        \n",
    "        # Get the device and dtype from the original layer\n",
    "        device = layer.weight.device\n",
    "        dtype = layer.weight.dtype\n",
    "        \n",
    "        # Split centroids into weights and bias\n",
    "        if layer.bias is not None:\n",
    "            # Last column of centroids represents the bias\n",
    "            compressed_weights = centroids[:, :-1]\n",
    "            compressed_bias = centroids[:, -1]\n",
    "        else:\n",
    "            compressed_weights = centroids\n",
    "            compressed_bias = None\n",
    "        \n",
    "        # Reshape compressed weights back to tensor form\n",
    "        compressed_weights_tensor = compressed_weights.reshape(K, in_channels, kernel_h, kernel_w)\n",
    "        \n",
    "        # Create compressed convolutional layer\n",
    "        compressed_layer = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=K,\n",
    "            kernel_size=layer.kernel_size,\n",
    "            stride=layer.stride,\n",
    "            padding=layer.padding,\n",
    "            dilation=layer.dilation,\n",
    "            groups=layer.groups,\n",
    "            bias=(compressed_bias is not None)\n",
    "        ).to(device)\n",
    "        \n",
    "        compressed_layer.weight.data = torch.tensor(compressed_weights_tensor, device=device, dtype=dtype)\n",
    "        \n",
    "        if compressed_bias is not None:\n",
    "            compressed_layer.bias.data = torch.tensor(compressed_bias, device=device, dtype=dtype)\n",
    "        else:\n",
    "            compressed_layer.bias = None\n",
    "            \n",
    "        return compressed_layer, P\n",
    "    \n",
    "\n",
    "    def _compress_holistically(self, current_layer, next_layer):\n",
    "        \"\"\"\n",
    "        (Private Helper) Compresses a hidden layer by clustering a representation \n",
    "        of its neurons that includes both incoming and outgoing weights.\n",
    "        Also measures the distortion.\n",
    "        \"\"\"\n",
    "        print(f\"--- Starting Holistic Compression on {current_layer} and {next_layer} ---\")\n",
    "        device = current_layer.weight.device\n",
    "        dtype = current_layer.weight.dtype\n",
    "        \n",
    "        # 1. IDENTIFY WEIGHTS\n",
    "        W1 = current_layer.weight.data.cpu().numpy()\n",
    "        b1 = current_layer.bias.data.cpu().numpy()\n",
    "        W2 = next_layer.weight.data.cpu().numpy()\n",
    "        \n",
    "        # print(W2)\n",
    "        # print()\n",
    "        # print(b1)\n",
    "        # print()\n",
    "     \n",
    "        # 2. COMBINE MATRICES\n",
    "        W2_T = W2.T\n",
    "        # print(\"W2\")\n",
    "        # print(W2_T)\n",
    "        # print()\n",
    "        \n",
    "        augmented_W1 = np.hstack([W1, b1.reshape(-1, 1)])\n",
    "        combined_matrix = np.concatenate([augmented_W1, W2_T], axis=1)\n",
    "        print(f\"Created combined matrix of shape: {combined_matrix.shape}\")\n",
    "    \n",
    "        # 3. AGGREGATE\n",
    "        num_neurons = combined_matrix.shape[0]\n",
    "        K = max(1, int(num_neurons * self.compression_ratio))\n",
    "        centroids, P = self.run_deterministic_annealing(combined_matrix, K)\n",
    "        print(f\"Aggregated into {K} centroids.\")\n",
    "\n",
    "        P_T = P.T\n",
    "        # print(\"\\n\",P_T)\n",
    "        \n",
    "        # 5. SEPARATE (Done before distortion measurement to get new weights)\n",
    "        new_aug_w1 = centroids[:, :augmented_W1.shape[1]]\n",
    "        new_w2_t = centroids[:, augmented_W1.shape[1]:]\n",
    "        new_w1 = new_aug_w1[:, :-1]\n",
    "        new_b1 = new_aug_w1[:, -1]\n",
    "        new_w2 = new_w2_t.T\n",
    "\n",
    "\n",
    "        # Original Weights multiplied by the association matrix\n",
    "        # p_sum = np.sum(P_T, axis = 1)\n",
    "        # p_sum = p_sum.reshape(-1,1)\n",
    "        # # print(p_sum)\n",
    "        # print()\n",
    "\n",
    "        # print(P.shape)\n",
    "        \n",
    "        # W1_M_P = ((P_T)@W1)/p_sum\n",
    "        # b1_M_P = ((P_T)@b1)/p_sum\n",
    "        # W2_M_P = (P_T@W2_T)/p_sum\n",
    "        # W2_M_P = W2_M_P.T\n",
    "\n",
    "        # new_w1 = W1_M_P\n",
    "        # new_b1 = b1_M_P\n",
    "        # new_w2 = W2_M_P\n",
    "        \n",
    "        # print(new_w1)\n",
    "        # print()\n",
    "        # print(\"W2_M_P\")\n",
    "        # print(W2_M_P)\n",
    "        # print()\n",
    "        # print(new_b1)\n",
    "        # print()\n",
    "        # print(\"new_w2\")\n",
    "        # print(new_w2)\n",
    "        # print()\n",
    "\n",
    "        # 4. MEASURE DISTORTION (now compares original vs reconstructed weights)\n",
    "        P_tensor = torch.tensor(P, device=device, dtype=dtype)\n",
    "        \n",
    "        # Reconstruct the original weight matrices from the new compressed ones\n",
    "        new_w1_tensor = torch.tensor(new_w1, device=device, dtype=dtype)\n",
    "        W1_recon_tensor = P_tensor @ new_w1_tensor\n",
    "\n",
    "        # print(W1_recon_tensor)\n",
    "        # print()\n",
    "        \n",
    "        new_w2_tensor = torch.tensor(new_w2, device=device, dtype=dtype)\n",
    "        # Note: The reconstruction for W2 uses P.T\n",
    "        W2_recon_tensor = new_w2_tensor @ P_tensor.T\n",
    "        \n",
    "        # Original weight tensors\n",
    "        W1_tensor = torch.tensor(W1, device=device, dtype=dtype)\n",
    "        W2_tensor = torch.tensor(W2, device=device, dtype=dtype)\n",
    "        \n",
    "        # Calculate dissimilarity using the provided function.\n",
    "        # The diagonal of the resulting distance matrix gives the squared distance\n",
    "        # between corresponding original and reconstructed neuron weights.\n",
    "        D1_matrix = self.calculate_distortion(W1_tensor, W1_recon_tensor)\n",
    "        dissimilarity_W1 = torch.sum(torch.diag(D1_matrix))\n",
    "        \n",
    "        D2_matrix = self.calculate_distortion(W2_tensor, W2_recon_tensor)\n",
    "        dissimilarity_W2 = torch.sum(torch.diag(D2_matrix))\n",
    "        \n",
    "        print(f\"Dissimilarity for W1 (incoming weights): {dissimilarity_W1.item():.4f}\")\n",
    "        print(f\"Dissimilarity for W2 (outgoing weights): {dissimilarity_W2.item():.4f}\")\n",
    "    \n",
    "        # 6. CREATE NEW LAYERS\n",
    "        new_current_layer = nn.Linear(in_features=new_w1.shape[1], out_features=new_w1.shape[0], bias=True)\n",
    "        new_current_layer.to(device)\n",
    "        new_current_layer.weight.data = torch.tensor(new_w1, device=device, dtype=dtype)\n",
    "        new_current_layer.bias.data = torch.tensor(new_b1, device=device, dtype=dtype)\n",
    "        \n",
    "        new_next_layer = nn.Linear(in_features=new_w2.shape[1], out_features=new_w2.shape[0], bias=(next_layer.bias is not None))\n",
    "        new_next_layer.to(device)\n",
    "        new_next_layer.weight.data = torch.tensor(new_w2, device=device, dtype=dtype)\n",
    "        if new_next_layer.bias is not None:\n",
    "            new_next_layer.bias.data = next_layer.bias.data.clone()\n",
    "            \n",
    "        print(\"--- Holistic Compression Complete ---\")\n",
    "        return new_current_layer, new_next_layer, P\n",
    "\n",
    "    \n",
    "    # def compress_model(self):\n",
    "    #     \"\"\"\n",
    "    #     Compress the entire neural network model, adjusting subsequent layers.\n",
    "    #     Now compresses all hidden layers including the first one (fc1),\n",
    "    #     while not compressing the input layer and output layer.\n",
    "    #     \"\"\"\n",
    "    #     compressed_model = deepcopy(self.model)\n",
    "        \n",
    "    #     # Try to move compressed model to device with error handling\n",
    "    #     try:\n",
    "    #         compressed_model.to(self.device)\n",
    "    #         print(\"Model successfully moved to device and Compressing...\")\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error moving compressed model to device: {e}\")\n",
    "    #         print(\"Falling back to CPU.\")\n",
    "    #         self.device = torch.device(\"cpu\")\n",
    "    #         compressed_model.to(self.device)\n",
    "    #         self.model.to(self.device)\n",
    "        \n",
    "    #     self.association_matrices = {}\n",
    "    #     prev_P = None\n",
    "    #     prev_layer_name = None\n",
    "\n",
    "        \n",
    "    #     # Get a list of all modules in the model\n",
    "    #     modules = list(compressed_model.named_modules())\n",
    "\n",
    "    #     # First pass: identify compressible layers\n",
    "    #     # Collect all linear and conv layers\n",
    "    #     linear_conv_layers = []\n",
    "    #     for name, module in modules:\n",
    "    #         if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "    #             linear_conv_layers.append((name, module))\n",
    "        \n",
    "    #     # The first layer is the input layer, the last layer is the output layer\n",
    "    #     input_layer_name = linear_conv_layers[0][0]\n",
    "    #     output_layer_name = linear_conv_layers[-1][0]\n",
    "        \n",
    "    #     print(f\"Input layer: {input_layer_name}\")\n",
    "    #     print(f\"Output layer: {output_layer_name}\")\n",
    "        \n",
    "    #     # Second pass: compress layers and adjust dimensions\n",
    "    #     for name, module in modules:\n",
    "    #         if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "    #             # Get parent module and attribute name\n",
    "    #             path = name.split('.')\n",
    "    #             parent = compressed_model\n",
    "    #             for p in path[:-1]:\n",
    "    #                 parent = getattr(parent, p)\n",
    "    #             attr_name = path[-1]\n",
    "    #             layer = getattr(parent, attr_name)\n",
    "                \n",
    "    #             # If this layer follows a compressed layer, adjust its input dimensions\n",
    "    #             if prev_P is not None:\n",
    "    #                 # Convert P to tensor and move to device\n",
    "    #                 try:\n",
    "    #                     P_tensor = torch.tensor(prev_P, dtype=torch.float32, device=self.device)\n",
    "    #                 except Exception as e:\n",
    "    #                     print(f\"Error creating P tensor on device: {e}\")\n",
    "    #                     print(\"Falling back to CPU for this operation.\")\n",
    "    #                     P_tensor = torch.tensor(prev_P, dtype=torch.float32, device='cpu')\n",
    "                    \n",
    "    #                 if isinstance(layer, nn.Linear):\n",
    "    #                     # Adjust input dimensions for linear layer\n",
    "    #                     original_weight = layer.weight.data\n",
    "    #                     new_weight = original_weight @ P_tensor\n",
    "                        \n",
    "    #                     # Create new linear layer with adjusted input dimensions\n",
    "    #                     new_in_features = prev_P.shape[1]\n",
    "    #                     new_layer = nn.Linear(new_in_features, layer.out_features, bias=(layer.bias is not None))\n",
    "    #                     new_layer.to(self.device)\n",
    "    #                     new_layer.weight.data = new_weight\n",
    "                        \n",
    "    #                     if layer.bias is not None:\n",
    "    #                         new_layer.bias.data = layer.bias.data.clone()\n",
    "                        \n",
    "    #                     # Replace the layer\n",
    "    #                     setattr(parent, attr_name, new_layer)\n",
    "    #                     layer = new_layer\n",
    "                \n",
    "    #             # Compress the current layer if it's not the output layer\n",
    "    #             # Now we compress all layers except the output layer\n",
    "    #             if name != output_layer_name:\n",
    "    #                 compressed_layer, P = self.compress_layer(layer)\n",
    "    #                 setattr(parent, attr_name, compressed_layer)\n",
    "                    \n",
    "    #                 # Store the association matrix for the next layer\n",
    "    #                 prev_P = P\n",
    "    #                 self.association_matrices[name] = P\n",
    "    #                 print(f\"Compressed layer {name}: {layer} -> {compressed_layer}\")\n",
    "    #             else:\n",
    "    #                 # Reset prev_P for non-compressible layers\n",
    "    #                 prev_P = None\n",
    "    #                 print(f\"Skipped layer {name} (output layer)\")\n",
    "\n",
    "    #      # Store compressed parameters and stats\n",
    "    #     self.compressed_params = {name: param.data.clone() for name, param in compressed_model.named_parameters()}\n",
    "    #     self.calculate_compression_stats()\n",
    "        \n",
    "    #     # Store the compressed model\n",
    "    #     self.compressed_model = compressed_model\n",
    "        \n",
    "    #     # Return the compressed model\n",
    "    #     return compressed_model\n",
    "\n",
    "\n",
    "    def compress_model(self):\n",
    "        \"\"\"\n",
    "        Compresses the entire model by iteratively applying the holistic, \n",
    "        two-layer compression strategy.\n",
    "        \"\"\"\n",
    "        compressed_model = deepcopy(self.model)\n",
    "        \n",
    "        # Helper functions to get/set modules by their string name (e.g., 'net.0')\n",
    "        def get_module_by_name(model, name):\n",
    "            path = name.split('.')\n",
    "            module = model\n",
    "            for p in path:\n",
    "                # Check for Sequential indexing\n",
    "                if p.isdigit():\n",
    "                    module = module[int(p)]\n",
    "                else:\n",
    "                    module = getattr(module, p)\n",
    "            return module\n",
    "    \n",
    "        def set_module_by_name(model, name, new_module):\n",
    "            path = name.split('.')\n",
    "            parent = model\n",
    "            for p in path[:-1]:\n",
    "                if p.isdigit():\n",
    "                    parent = parent[int(p)]\n",
    "                else:\n",
    "                    parent = getattr(parent, p)\n",
    "            \n",
    "            attr_name = path[-1]\n",
    "            if attr_name.isdigit():\n",
    "                parent[int(attr_name)] = new_module\n",
    "            else:\n",
    "                setattr(parent, attr_name, new_module)\n",
    "    \n",
    "        # Get a list of the names of all compressible layers\n",
    "        compressible_layer_names = []\n",
    "        for name, module in compressed_model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                compressible_layer_names.append(name)\n",
    "        \n",
    "        if len(compressible_layer_names) < 2:\n",
    "            print(\"Not enough compressible layers to apply holistic compression.\")\n",
    "            return compressed_model\n",
    "    \n",
    "        print(f\"Found compressible layers: {compressible_layer_names}\")\n",
    "    \n",
    "        # Iterate through the layers in overlapping pairs, e.g., (L0, L2), then (L2, L4)\n",
    "        # We stop before the last layer, as it cannot be a \"current_layer\" with a \"next_layer\".\n",
    "        for i in range(1,len(compressible_layer_names) - 1):\n",
    "        # for i in range(1,2):\n",
    "            current_layer_name = compressible_layer_names[i]\n",
    "            next_layer_name = compressible_layer_names[i+1]\n",
    "            \n",
    "            print(f\"\\nProcessing pair: ({current_layer_name}, {next_layer_name})\")\n",
    "            \n",
    "            # Get the actual layer modules from the potentially modified compressed_model\n",
    "            current_layer = get_module_by_name(compressed_model, current_layer_name)\n",
    "            next_layer = get_module_by_name(compressed_model, next_layer_name)\n",
    "    \n",
    "            # Perform the holistic compression on the current pair\n",
    "            new_current_layer, new_next_layer, P = self._compress_holistically(current_layer, next_layer)\n",
    "            \n",
    "            # Replace these two layers in the compressed_model\n",
    "            set_module_by_name(compressed_model, current_layer_name, new_current_layer)\n",
    "            set_module_by_name(compressed_model, next_layer_name, new_next_layer)\n",
    "            \n",
    "            self.association_matrices[current_layer_name] = P\n",
    "            print(f\"Updated layers '{current_layer_name}' and '{next_layer_name}' in the model.\")\n",
    "    \n",
    "        # Store final model details\n",
    "        print(\"\\nModel compression complete.\")\n",
    "        self.compressed_params = {name: param.data.clone() for name, param in compressed_model.named_parameters()}\n",
    "        self.calculate_compression_stats()\n",
    "        self.compressed_model = compressed_model\n",
    "        \n",
    "        return compressed_model\n",
    "\n",
    "    \n",
    "        \n",
    "    def calculate_compression_stats(self):\n",
    "        \"\"\"\n",
    "        Calculate and store compression statistics.\n",
    "        \"\"\"\n",
    "        original_params = sum(p.numel() for p in self.original_params.values())\n",
    "        compressed_params = sum(p.numel() for p in self.compressed_params.values())\n",
    "        \n",
    "        self.compression_stats = {\n",
    "            'original_parameters': original_params,\n",
    "            'compressed_parameters': compressed_params,\n",
    "            'compression_ratio': 1 - (compressed_params / original_params),\n",
    "            'parameter_reduction': original_params - compressed_params\n",
    "        }\n",
    "    \n",
    "    def evaluate_model(self, model, test_loader):\n",
    "        \"\"\"\n",
    "        Evaluate the model on a test dataset using GPU.\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch model to evaluate\n",
    "            test_loader: DataLoader for test dataset\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing evaluation metrics\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        model.to(self.device)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        criterion = nn.MSELoss().to(self.device)  # Use MSE for regression\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets).item() * inputs.size(0)\n",
    "                total_loss += loss\n",
    "                total_samples += inputs.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / total_samples\n",
    "        \n",
    "        return {\n",
    "            'loss': avg_loss\n",
    "        }\n",
    "    \n",
    "    def compare_models(self, test_loader):\n",
    "        \"\"\"\n",
    "        Compare the original and compressed models on a test dataset using GPU.\n",
    "        \n",
    "        Args:\n",
    "            test_loader: DataLoader for test dataset\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing comparison metrics\n",
    "        \"\"\"\n",
    "        # Evaluate original model\n",
    "        original_results = self.evaluate_model(self.model, test_loader)\n",
    "        \n",
    "        # Evaluate compressed model\n",
    "        if self.compressed_model is None:\n",
    "            compressed_model = self.compress_model()\n",
    "        else:\n",
    "            compressed_model = self.compressed_model\n",
    "            \n",
    "        compressed_results = self.evaluate_model(compressed_model, test_loader)\n",
    "        \n",
    "        # Calculate differences\n",
    "        loss_diff = compressed_results['loss'] - original_results['loss']\n",
    "        \n",
    "        return {\n",
    "            'original_loss': original_results['loss'],\n",
    "            'compressed_loss': compressed_results['loss'],\n",
    "            'loss_difference': loss_diff,\n",
    "            **self.compression_stats\n",
    "        }\n",
    "    \n",
    "    def visualize_compression(self, layer_name=None):\n",
    "        \"\"\"\n",
    "        Visualize the compression of a specific layer or the entire model.\n",
    "        \n",
    "        Args:\n",
    "            layer_name: Name of the layer to visualize (if None, visualize all layers)\n",
    "        \"\"\"\n",
    "        if layer_name:\n",
    "            # Visualize a specific layer\n",
    "            if layer_name in self.original_params and layer_name in self.compressed_params:\n",
    "                original_weights = self.original_params[layer_name].cpu().numpy()\n",
    "                compressed_weights = self.compressed_params[layer_name].cpu().numpy()\n",
    "                \n",
    "                plt.figure(figsize=(12, 6))\n",
    "                \n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.imshow(original_weights, cmap='viridis')\n",
    "                plt.title(f'Original Weights - {layer_name}')\n",
    "                plt.colorbar()\n",
    "                \n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.imshow(compressed_weights, cmap='viridis')\n",
    "                plt.title(f'Compressed Weights - {layer_name}')\n",
    "                plt.colorbar()\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(f\"Layer {layer_name} not found in model parameters.\")\n",
    "        else:\n",
    "            # Visualize compression statistics\n",
    "            stats = self.compression_stats\n",
    "            if stats:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                \n",
    "                plt.subplot(1, 2, 1)\n",
    "                categories = ['Original', 'Compressed']\n",
    "                values = [stats['original_parameters'], stats['compressed_parameters']]\n",
    "                plt.bar(categories, values)\n",
    "                plt.title('Parameter Count')\n",
    "                plt.ylabel('Number of Parameters')\n",
    "                \n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.pie([stats['compression_ratio'], 1 - stats['compression_ratio']], \n",
    "                        labels=['Compressed', 'Remaining'], autopct='%1.1f%%')\n",
    "                plt.title('Compression Ratio')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "    \n",
    "\n",
    "    def model_to_graph(self, model):\n",
    "        \"\"\"\n",
    "        Convert a neural network model to a graph representation.\n",
    "        Includes bias neurons as special nodes.\n",
    "        Creates all edges without filtering by weight.\n",
    "        Now includes the input layer with dynamic size.\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch model\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (networkx.DiGraph, dict) - Graph and node positions\n",
    "        \"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        node_positions = {}\n",
    "        layer_counts = {}\n",
    "        layer_nodes = {}\n",
    "        \n",
    "        # Add input layer\n",
    "        input_layer_name = \"input\"\n",
    "        \n",
    "        # Determine input size dynamically from the first layer\n",
    "        first_layer = None\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                first_layer = module\n",
    "                break\n",
    "        \n",
    "        if first_layer is not None:\n",
    "            input_size = first_layer.in_features\n",
    "            print(f\"Detected input size: {input_size}\")\n",
    "        else:\n",
    "            # Default fallback\n",
    "            input_size = 3  # Default for your model\n",
    "            print(f\"Using default input size: {input_size}\")\n",
    "        \n",
    "        layer_counts[input_layer_name] = input_size\n",
    "        \n",
    "        # First pass: count nodes per layer\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                layer_counts[name] = module.out_features\n",
    "            elif isinstance(module, nn.Conv2d):\n",
    "                layer_counts[name] = module.out_features\n",
    "        \n",
    "        # Print layer information for debugging\n",
    "        print(\"\\nLayer information for model:\")\n",
    "        for name, count in layer_counts.items():\n",
    "            print(f\"{name}: {count} nodes\")\n",
    "        \n",
    "        # Add input layer nodes\n",
    "        layer_idx = 0\n",
    "        nodes = []\n",
    "        for i in range(input_size):\n",
    "            node_id = f\"{input_layer_name}_{i}\"\n",
    "            G.add_node(node_id, layer=input_layer_name, index=i, type='input')\n",
    "            nodes.append(node_id)\n",
    "            \n",
    "            # Position nodes in a grid\n",
    "            x = layer_idx * 2\n",
    "            y = (i - input_size/2) * 0.5\n",
    "            node_positions[node_id] = (x, y)\n",
    "        \n",
    "        layer_nodes[input_layer_name] = nodes\n",
    "        layer_idx += 1\n",
    "        \n",
    "        # Add a single bias node for the entire network\n",
    "        bias_node_id = \"bias\"\n",
    "        G.add_node(bias_node_id, layer=\"bias\", type='bias', value=1.0)\n",
    "        x_bias = 0  # Position bias node to the left of all layers\n",
    "        y_bias = 0\n",
    "        node_positions[bias_node_id] = (x_bias, y_bias)\n",
    "        \n",
    "        # Second pass: create nodes and edges for the rest of the model\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                # Create nodes for this layer\n",
    "                nodes = []\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    num_nodes = module.out_features\n",
    "                else:  # Conv2d\n",
    "                    num_nodes = module.out_features\n",
    "                \n",
    "                for i in range(num_nodes):\n",
    "                    node_id = f\"{name}_{i}\"\n",
    "                    G.add_node(node_id, layer=name, index=i, type='neuron')\n",
    "                    nodes.append(node_id)\n",
    "                    \n",
    "                    # Position nodes in a grid\n",
    "                    x = layer_idx * 2\n",
    "                    y = (i - num_nodes/2) * 0.5\n",
    "                    node_positions[node_id] = (x, y)\n",
    "                \n",
    "                layer_nodes[name] = nodes\n",
    "                layer_idx += 1\n",
    "        \n",
    "        # Create edges between consecutive layers\n",
    "        layer_names = list(layer_nodes.keys())\n",
    "        print(f\"\\nLayer names: {layer_names}\")\n",
    "        \n",
    "        for i in range(len(layer_names) - 1):\n",
    "            current_layer = layer_names[i]\n",
    "            next_layer = layer_names[i+1]\n",
    "            \n",
    "            print(f\"\\nCreating edges from {current_layer} to {next_layer}\")\n",
    "            \n",
    "            if current_layer == \"input\":\n",
    "                # Create edges from input layer to first hidden layer\n",
    "                first_hidden_module = dict(model.named_modules())[layer_names[i+1]]\n",
    "                \n",
    "                if isinstance(first_hidden_module, nn.Linear):\n",
    "                    weights = first_hidden_module.weight.data.cpu().numpy()\n",
    "                    bias = first_hidden_module.bias.data.cpu().numpy() if first_hidden_module.bias is not None else None\n",
    "                    \n",
    "                    print(f\"Weight matrix shape: {weights.shape}\")\n",
    "                    print(f\"Max absolute weight: {np.max(np.abs(weights))}\")\n",
    "                    \n",
    "                    # Connect input neurons to hidden neurons\n",
    "                    for j, next_node in enumerate(layer_nodes[next_layer]):\n",
    "                        for k, current_node in enumerate(layer_nodes[current_layer]):\n",
    "                            # Check if indices are within bounds\n",
    "                            if j < weights.shape[0] and k < weights.shape[1]:\n",
    "                                weight = weights[j, k]\n",
    "                                # Add all edges regardless of weight\n",
    "                                G.add_edge(current_node, next_node, weight=weight)\n",
    "                    \n",
    "                    # Connect bias to all hidden neurons\n",
    "                    if bias is not None:\n",
    "                        print(f\"Bias shape: {bias.shape}\")\n",
    "                        print(f\"Max absolute bias: {np.max(np.abs(bias))}\")\n",
    "                        \n",
    "                        for j, next_node in enumerate(layer_nodes[next_layer]):\n",
    "                            if j < bias.shape[0]:\n",
    "                                bias_weight = bias[j]\n",
    "                                # Connect from the single bias node\n",
    "                                G.add_edge(bias_node_id, next_node, weight=bias_weight)\n",
    "            else:\n",
    "                # Create edges between regular layers using NEXT layer's weights\n",
    "                next_module = dict(model.named_modules())[next_layer]\n",
    "            \n",
    "                if isinstance(next_module, nn.Linear):\n",
    "                    weights = next_module.weight.data.cpu().numpy()\n",
    "                    bias = next_module.bias.data.cpu().numpy() if next_module.bias is not None else None\n",
    "            \n",
    "                    print(f\"Weight matrix shape: {weights.shape}\")\n",
    "                    print(f\"Max absolute weight: {np.max(np.abs(weights))}\")\n",
    "            \n",
    "                    # weights[j, k]: from current_layer neuron k -> next_layer neuron j\n",
    "                    for j, next_node in enumerate(layer_nodes[next_layer]):\n",
    "                        for k, current_node in enumerate(layer_nodes[current_layer]):\n",
    "                            if j < weights.shape[0] and k < weights.shape[1]:\n",
    "                                w = weights[j, k]\n",
    "                                G.add_edge(current_node, next_node, weight=w)\n",
    "            \n",
    "                    # Bias of NEXT layer connects to its neurons\n",
    "                    if bias is not None:\n",
    "                        print(f\"Bias shape: {bias.shape}\")\n",
    "                        print(f\"Max absolute bias: {np.max(np.abs(bias))}\")\n",
    "                        for j, next_node in enumerate(layer_nodes[next_layer]):\n",
    "                            if j < bias.shape[0]:\n",
    "                                bw = bias[j]\n",
    "                                G.add_edge(\"bias\", next_node, weight=bw)\n",
    "            \n",
    "                elif isinstance(next_module, nn.Conv2d):\n",
    "                    # Same idea: use next_module, not current_module\n",
    "                    weights = next_module.weight.data.cpu().numpy()\n",
    "                    bias = next_module.bias.data.cpu().numpy() if next_module.bias is not None else None\n",
    "                    out_c, in_c, kh, kw = weights.shape\n",
    "            \n",
    "                    print(f\"Weight tensor shape: {weights.shape}\")\n",
    "                    print(f\"Max absolute weight: {np.max(np.abs(weights))}\")\n",
    "            \n",
    "                    for j, next_node in enumerate(layer_nodes[next_layer]):\n",
    "                        for k, current_node in enumerate(layer_nodes[current_layer]):\n",
    "                            if j < out_c and k < in_c:\n",
    "                                avg_w = np.mean(weights[j, k])\n",
    "                                G.add_edge(current_node, next_node, weight=avg_w)\n",
    "            \n",
    "                    if bias is not None:\n",
    "                        print(f\"Bias shape: {bias.shape}\")\n",
    "                        print(f\"Max absolute bias: {np.max(np.abs(bias))}\")\n",
    "                        for j, next_node in enumerate(layer_nodes[next_layer]):\n",
    "                            if j < bias.shape[0]:\n",
    "                                bw = bias[j]\n",
    "                                G.add_edge(\"bias\", next_node, weight=bw)\n",
    "        \n",
    "        print(f\"\\nGraph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "        return G, node_positions\n",
    "    \n",
    "    def draw_neural_network(self, G, pos, title=\"Neural Network\", \n",
    "                           edge_percentage=0.01, min_edges=100, max_edges=5000,\n",
    "                           min_alpha=0.1, max_width=3.0, min_width=0.5):\n",
    "        \"\"\"\n",
    "        Draw neural network with proper sign coloring and edge thickness.\n",
    "        - Blue = positive weights\n",
    "        - Red = negative weights\n",
    "        - Thickness  |weight|\n",
    "        - If params <= 1000  draw all edges, else sample subset\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        plt.figure(figsize=(14, 10))\n",
    "    \n",
    "        # Extract edges and weights\n",
    "        edges = list(G.edges())\n",
    "        weights = np.array([G[u][v]['weight'] for u, v in edges])\n",
    "        abs_weights = np.abs(weights)\n",
    "    \n",
    "        # Determine if full or partial visualization\n",
    "        total_edges = len(edges)\n",
    "        num_params = total_edges  # edges ~ params for visualization\n",
    "        if num_params <= 1000:\n",
    "            selected_idx = np.arange(total_edges)\n",
    "            print(f\"Drawing all {total_edges} edges.\")\n",
    "        else:\n",
    "            num_edges_to_draw = max(int(total_edges * edge_percentage), min_edges)\n",
    "            num_edges_to_draw = min(num_edges_to_draw, total_edges)\n",
    "            print(f\"Sampling {num_edges_to_draw}/{total_edges} edges ({100*num_edges_to_draw/total_edges:.1f}%)\")\n",
    "            selected_idx = np.random.choice(total_edges, size=num_edges_to_draw, replace=False)\n",
    "    \n",
    "        selected_edges = [edges[i] for i in selected_idx]\n",
    "        selected_weights = weights[selected_idx]\n",
    "        selected_abs = abs_weights[selected_idx]\n",
    "    \n",
    "        # Normalize for visual scaling\n",
    "        max_w = np.max(selected_abs) if np.max(selected_abs) > 0 else 1\n",
    "        normalized = selected_abs / max_w\n",
    "    \n",
    "        edge_widths = min_width + (max_width - min_width) * normalized\n",
    "        edge_alphas = min_alpha + (1.0 - min_alpha) * normalized\n",
    "        edge_colors = ['blue' if w > 0 else 'red' for w in selected_weights]\n",
    "    \n",
    "        # Draw nodes by type\n",
    "        input_nodes = [n for n in G.nodes if G.nodes[n].get('type') == 'input']\n",
    "        neuron_nodes = [n for n in G.nodes if G.nodes[n].get('type') == 'neuron']\n",
    "        bias_nodes = [n for n in G.nodes if G.nodes[n].get('type') == 'bias']\n",
    "    \n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=input_nodes, node_size=200, node_color='lightcoral', alpha=0.9)\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=neuron_nodes, node_size=200, node_color='lightgreen', alpha=0.9)\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=bias_nodes, node_size=200, node_color='lightblue', alpha=0.9)\n",
    "    \n",
    "        # Draw edges\n",
    "        for (u, v), c, w, a in zip(selected_edges, edge_colors, edge_widths, edge_alphas):\n",
    "            nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], edge_color=c, width=w, alpha=a, arrows=True, arrowsize=10)\n",
    "    \n",
    "        # Label layers\n",
    "        layer_positions = {}\n",
    "        for node, data in G.nodes(data=True):\n",
    "            layer = data.get('layer', '')\n",
    "            layer_positions.setdefault(layer, []).append(pos[node])\n",
    "    \n",
    "        for layer, pts in layer_positions.items():\n",
    "            x = np.mean([p[0] for p in pts])\n",
    "            y = max([p[1] for p in pts]) + 0.8\n",
    "            plt.text(x, y, layer, ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "        plt.title(title, fontsize=16)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"Rendered in {duration:.2f}s\")\n",
    "        plt.show()\n",
    "        return duration\n",
    "    \n",
    "    def visualize_networks(self, edge_percentage=0.01, min_edges=100, max_edges=5000,\n",
    "                          min_alpha=0.1, max_width=3.0, min_width=0.5):\n",
    "        \"\"\"\n",
    "        Visualize both the original and compressed neural networks.\n",
    "        Includes bias neurons as special nodes.\n",
    "        Only draws a percentage of edges with the highest absolute weights.\n",
    "        Now includes the input layer with dynamic size.\n",
    "        \n",
    "        Args:\n",
    "            edge_percentage: Percentage of edges to draw (0-1)\n",
    "            min_edges: Minimum number of edges to draw\n",
    "            max_edges: Maximum number of edges to draw\n",
    "            min_alpha: Minimum opacity for edges (smallest weights)\n",
    "            max_width: Maximum width for edges (largest weights)\n",
    "            min_width: Minimum width for edges (smallest weights)\n",
    "        \"\"\"\n",
    "        # Create compressed model if not already created\n",
    "        if self.compressed_model is None:\n",
    "            self.compress_model()\n",
    "        \n",
    "        # Convert models to graphs\n",
    "        try:\n",
    "            print(\"Creating graph for original model...\")\n",
    "            original_graph, original_pos = self.model_to_graph(self.model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating graph for original model: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            print(\"Creating graph for compressed model...\")\n",
    "            compressed_graph, compressed_pos = self.model_to_graph(self.compressed_model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating graph for compressed model: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return\n",
    "        \n",
    "        # Draw original network\n",
    "        try:\n",
    "            print(\"Drawing original network...\")\n",
    "            original_duration = self.draw_neural_network(\n",
    "                original_graph, \n",
    "                original_pos, \n",
    "                title=\"Original Neural Network\",\n",
    "                edge_percentage=edge_percentage,\n",
    "                min_edges=min_edges,\n",
    "                max_edges=max_edges,\n",
    "                min_alpha=min_alpha,\n",
    "                max_width=max_width,\n",
    "                min_width=min_width\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error drawing original network: {e}\")\n",
    "            traceback.print_exc()\n",
    "            original_duration = 0\n",
    "        \n",
    "        # Draw compressed network\n",
    "        try:\n",
    "            print(\"Drawing compressed network...\")\n",
    "            compressed_duration = self.draw_neural_network(\n",
    "                compressed_graph, \n",
    "                compressed_pos, \n",
    "                title=\"Compressed Neural Network\",\n",
    "                edge_percentage=edge_percentage,\n",
    "                min_edges=min_edges,\n",
    "                max_edges=max_edges,\n",
    "                min_alpha=min_alpha,\n",
    "                max_width=max_width,\n",
    "                min_width=min_width\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error drawing compressed network: {e}\")\n",
    "            traceback.print_exc()\n",
    "            compressed_duration = 0\n",
    "        \n",
    "        # Print compression statistics\n",
    "        print(\"\\nCompression Statistics:\")\n",
    "        for key, value in self.compression_stats.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        \n",
    "        # Print timing statistics\n",
    "        print(\"\\nTiming Statistics:\")\n",
    "        print(f\"Original graph drawing time: {original_duration:.2f} seconds\")\n",
    "        print(f\"Compressed graph drawing time: {compressed_duration:.2f} seconds\")\n",
    "        print(f\"Total drawing time: {original_duration + compressed_duration:.2f} seconds\")\n",
    "\n",
    "    def visualize_trained_network(self, model, max_nodes_per_layer=200):\n",
    "        \"\"\"\n",
    "        Visualize a fully-connected feedforward network from its learned weights.\n",
    "        Colors represent sign (red=negative, blue=positive),\n",
    "        and edge width represents magnitude.\n",
    "        \n",
    "        Args:\n",
    "            model: torch.nn.Module with nn.Linear layers\n",
    "            max_nodes_per_layer: limit nodes for clarity in large layers\n",
    "        \"\"\"\n",
    "        layers = [module for module in model.modules() if isinstance(module, torch.nn.Linear)]\n",
    "        n_layers = len(layers) + 1\n",
    "        layer_sizes = [layers[0].in_features] + [l.out_features for l in layers]\n",
    "    \n",
    "        # Normalize node count for drawing clarity\n",
    "        display_sizes = [min(s, max_nodes_per_layer) for s in layer_sizes]\n",
    "        \n",
    "        # Setup figure\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(\"Learned Neural Network Structure\", fontsize=14, weight='bold')\n",
    "    \n",
    "        # Position nodes\n",
    "        layer_positions = []\n",
    "        x_spacing = 1 / (len(display_sizes) - 1)\n",
    "        for i, n_nodes in enumerate(display_sizes):\n",
    "            y_spacing = 1 / (n_nodes + 1)\n",
    "            layer_positions.append([(i * x_spacing, 1 - (j + 1) * y_spacing) for j in range(n_nodes)])\n",
    "        \n",
    "        # Draw edges\n",
    "        for i, layer in enumerate(layers):\n",
    "            W = layer.weight.data.cpu().numpy()\n",
    "            # Clip large layers for clarity if truncated\n",
    "            W = W[:display_sizes[i+1], :display_sizes[i]]\n",
    "            \n",
    "            max_w = np.max(np.abs(W)) + 1e-8\n",
    "            for out_idx, out_pos in enumerate(layer_positions[i+1]):\n",
    "                for in_idx, in_pos in enumerate(layer_positions[i]):\n",
    "                    w = W[out_idx, in_idx]\n",
    "                    color = 'blue' if w > 0 else 'red'\n",
    "                    alpha = np.clip(abs(w) / max_w, 0.1, 1.0)\n",
    "                    lw = 0.5 + 3 * abs(w) / max_w\n",
    "                    ax.plot([in_pos[0], out_pos[0]], [in_pos[1], out_pos[1]],\n",
    "                            color=color, alpha=alpha, linewidth=lw, zorder=1)\n",
    "    \n",
    "        # Draw nodes\n",
    "        for layer in layer_positions:\n",
    "            for (x, y) in layer:\n",
    "                circle = plt.Circle((x, y), 0.01, color='lightgray', ec='k', zorder=3)\n",
    "                ax.add_artist(circle)\n",
    "    \n",
    "        # Annotate layers\n",
    "        for i, size in enumerate(layer_sizes):\n",
    "            ax.text(i * x_spacing, 1.05, f\"Layer {i}\\n({size})\",\n",
    "                    ha='center', fontsize=10, weight='bold')\n",
    "    \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d23555",
   "metadata": {
    "papermill": {
     "duration": 0.003282,
     "end_time": "2025-11-09T21:55:46.695527",
     "exception": false,
     "start_time": "2025-11-09T21:55:46.692245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbcb2cc",
   "metadata": {
    "_cell_guid": "5cd84c08-d813-4509-84fe-9c01113fe1ce",
    "_uuid": "b92af156-8b6d-4695-a75d-46eaecc0c43e",
    "execution": {
     "iopub.execute_input": "2025-11-09T21:55:46.702949Z",
     "iopub.status.busy": "2025-11-09T21:55:46.702694Z",
     "iopub.status.idle": "2025-11-09T22:34:42.261072Z",
     "shell.execute_reply": "2025-11-09T22:34:42.260240Z"
    },
    "papermill": {
     "duration": 2335.563826,
     "end_time": "2025-11-09T22:34:42.262492",
     "exception": false,
     "start_time": "2025-11-09T21:55:46.698666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the saved weights\n",
    "try:\n",
    "    model = torch.load(\"/kaggle/input/single-layer-models/F1_model_full.pth\",map_location=device, weights_only = False)\n",
    "    print(\"Model loaded.\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading:\", e)\n",
    "\n",
    "\n",
    "\n",
    "# Test Script\n",
    "CRs = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "pert = [0.0005, 0.001, 0.005, 0.01]\n",
    "results_df = pd.DataFrame(columns=[\n",
    "     'Comp ratio',\n",
    "     'Temp',\n",
    "     'perturb',\n",
    "     'val_loss',\n",
    "     'time_seconds'\n",
    "])\n",
    "results_file_path = 'compression_test_results.csv'\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Starting Hyperparameter Test Run\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for ratio in CRs:\n",
    "    for perturb in pert:\n",
    "        print(f\"\\n--- Testing: Ratio={ratio}, Perturb={perturb} ---\")\n",
    "        result_data = {\n",
    "                    'Comp ratio': ratio,\n",
    "                    'Temp' : 80,\n",
    "                    'perturb': perturb\n",
    "        }\n",
    "        start_time = time.time()\n",
    "\n",
    "            \n",
    "        # Evaluate original model\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                preds = model(xb)\n",
    "                val_loss += criterion(preds,yb).item() * xb.size(0)\n",
    "                val_total += yb.size(0)\n",
    "        \n",
    "        print(f'\\nOriginal Model loss: {val_loss / val_total:.5f}')\n",
    "        \n",
    "        \n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if \"weight\" in name:\n",
    "        #         print(f\"\\n{name} shape: {param.shape}\")\n",
    "        #         print(param)  # raw tensor\n",
    "        \n",
    "        # Create compressor with CUDA support\n",
    "        try:\n",
    "            compressor = NeuralNetworkCompressor(model, T = 80, PERTURB = perturb,  compression_ratio=ratio, device=device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating compressor: {e}\")\n",
    "            traceback.print_exc()\n",
    "            # If CUDA fails, try with CPU\n",
    "            device = torch.device(\"cpu\")\n",
    "            compressor = NeuralNetworkCompressor(model, T = 80, PERTURB = perturb, compression_ratio=ratio, device=device)\n",
    "        \n",
    "        # Compress the model\n",
    "        try:\n",
    "            compressed_model = compressor.compress_model()\n",
    "            print(\"Compressed model created and stored.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during compression: {e}\")\n",
    "            traceback.print_exc()\n",
    "            exit(1)\n",
    "        \n",
    "        # Print compressed model architecture\n",
    "        print(\"\\nCompressed model architecture:\")\n",
    "        for name, param in compressed_model.named_parameters():\n",
    "            print(f\"{name}: {param.shape}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        # Evaluate the compressed model\n",
    "        compressed_model.eval()\n",
    "        comp_val_loss = 0\n",
    "        comp_val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                preds = compressed_model(xb)\n",
    "                comp_val_loss += criterion(preds,yb).item() * xb.size(0)\n",
    "                comp_val_total += yb.size(0)\n",
    "        print(f'\\nCompressed Model loss: {comp_val_loss/ comp_val_total:.5f}')\n",
    "        \n",
    "        final_loss = comp_val_loss/ comp_val_total\n",
    "        \n",
    "        # Compare models\n",
    "        # try:\n",
    "        #     comparison = compressor.compare_models(test_loader)\n",
    "        #     print(\"\\nCompression Statistics:\")\n",
    "        #     for key, value in comparison.items():\n",
    "        #         print(f\"{key}: {value}\")\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error during model comparison: {e}\")\n",
    "        #     traceback.print_exc()\n",
    "        \n",
    "        compressor.visualize_trained_network(model)\n",
    "        compressor.visualize_trained_network(compressed_model)\n",
    "        # Visualize compression with optimized edge sampling\n",
    "        # try:\n",
    "        #     # For large graphs, use a smaller percentage of edges\n",
    "        #     compressor.visualize_networks(\n",
    "        #         edge_percentage=1,  # 0.5% of edges\n",
    "        #         min_edges=100,        # At least 100 edges\n",
    "        #         max_edges=2000,       # At most 2000 edges\n",
    "        #         min_alpha=0.1,        # Minimum opacity\n",
    "        #         max_width=3.0,        # Maximum edge width\n",
    "        #         min_width=0.5         # Minimum edge width\n",
    "        #     )\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error during visualization: {e}\")\n",
    "        #     traceback.print_exc()\n",
    "\n",
    "        result_data['val_loss'] = final_loss\n",
    "        result_data['time_seconds'] = duration\n",
    "\n",
    "        new_row_df = pd.DataFrame([result_data])\n",
    "        results_df = pd.concat([results_df, new_row_df], ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"\\n--- All test runs complete. ---\")\n",
    "if not results_df.empty:\n",
    "    results_df.to_csv(results_file_path, index=False)\n",
    "    print(f\"Results saved to '{results_file_path}'\")\n",
    "    print(\"\\nFinal Results Summary:\")\n",
    "    print(results_df)\n",
    "else:\n",
    "    print(\"No results were logged.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f02c1c9",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-09T22:34:43.162481Z",
     "iopub.status.busy": "2025-11-09T22:34:43.162048Z",
     "iopub.status.idle": "2025-11-09T22:34:43.468709Z",
     "shell.execute_reply": "2025-11-09T22:34:43.467828Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.759696,
     "end_time": "2025-11-09T22:34:43.470185",
     "exception": false,
     "start_time": "2025-11-09T22:34:42.710489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"bias\" in name:\n",
    "        print(f\"\\n{name} shape: {param.shape}\")\n",
    "        print(param)  # raw tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a48379",
   "metadata": {
    "_cell_guid": "6dff04c2-df04-48b3-9e09-a9b8f3a6a172",
    "_uuid": "6fe0b0a8-7dc2-45f0-ac1d-a9dbba67dd9a",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-09T22:34:44.365402Z",
     "iopub.status.busy": "2025-11-09T22:34:44.365125Z",
     "iopub.status.idle": "2025-11-09T22:34:44.374349Z",
     "shell.execute_reply": "2025-11-09T22:34:44.373528Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.448317,
     "end_time": "2025-11-09T22:34:44.375588",
     "exception": false,
     "start_time": "2025-11-09T22:34:43.927271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, param in compressed_model.named_parameters():\n",
    "    if \"bias\" in name:\n",
    "        print(f\"\\n{name} shape: {param.shape}\")\n",
    "        print(param)  # raw tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143bf49f",
   "metadata": {
    "papermill": {
     "duration": 0.434551,
     "end_time": "2025-11-09T22:34:45.246917",
     "exception": false,
     "start_time": "2025-11-09T22:34:44.812366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a6ffd",
   "metadata": {
    "papermill": {
     "duration": 0.439221,
     "end_time": "2025-11-09T22:34:46.122624",
     "exception": false,
     "start_time": "2025-11-09T22:34:45.683403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c07da9",
   "metadata": {
    "papermill": {
     "duration": 0.447327,
     "end_time": "2025-11-09T22:34:47.020804",
     "exception": false,
     "start_time": "2025-11-09T22:34:46.573477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 274868732,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2368.813366,
   "end_time": "2025-11-09T22:34:50.629388",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-09T21:55:21.816022",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
