{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def init_mlp_weights(sizes, std=0.6):\n",
    "    \"\"\"Return [W1..W4] with Wi.shape = (sizes[i+1], sizes[i])\"\"\"\n",
    "    weights = []\n",
    "    for i in range(len(sizes)-1):\n",
    "        W = torch.randn(sizes[i+1], sizes[i]) * std\n",
    "        weights.append(W)\n",
    "    return weights\n",
    "\n",
    "def draw_network(weights, sizes, title=\"Network\", max_edges_per_pair=800):\n",
    "    \"\"\"Simple layered plot; edge width ∝ |weight| (largest edges kept if too many).\"\"\"\n",
    "    L = len(sizes)\n",
    "    xs = np.linspace(0, 1, L)\n",
    "    node_pos = []\n",
    "    for li, n in enumerate(sizes):\n",
    "        ys = np.linspace(0, 1, n)\n",
    "        node_pos.append([(xs[li], y) for y in ys])\n",
    "\n",
    "    plt.figure(figsize=(11, 6))\n",
    "    ax = plt.gca(); ax.set_title(title); ax.set_axis_off()\n",
    "\n",
    "    for li, W in enumerate(weights):\n",
    "        src = node_pos[li]; dst = node_pos[li+1]\n",
    "        Wnp = W.detach().cpu().numpy()\n",
    "        rows, cols = Wnp.shape\n",
    "        edges = [(i, j, Wnp[j, i]) for j in range(rows) for i in range(cols)]\n",
    "        if len(edges) > max_edges_per_pair:\n",
    "            edges.sort(key=lambda t: abs(t[2]), reverse=True)\n",
    "            edges = edges[:max_edges_per_pair]\n",
    "        if edges:\n",
    "            mags = np.array([abs(e[2]) for e in edges])\n",
    "            widths = 0.2 + 2.8 * (mags / (mags.max() + 1e-12))\n",
    "            for (i, j, w), lw in zip(edges, widths):\n",
    "                x1, y1 = src[i]; x2, y2 = dst[j]\n",
    "                ax.plot([x1, x2], [y1, y2], linewidth=lw, alpha=0.6)\n",
    "\n",
    "    for pos in node_pos:\n",
    "        ax.scatter([p[0] for p in pos], [p[1] for p in pos], s=60, zorder=3)\n",
    "    plt.show()\n",
    "\n",
    "def frob(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.linalg.norm(x, ord='fro')\n",
    "\n",
    "def end_to_end_map(weights):\n",
    "    \"\"\"Linearized input→output map.\"\"\"\n",
    "    A = weights[-1]\n",
    "    for W in reversed(weights[:-1]):\n",
    "        A = A @ W\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DA-style soft clustering (rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _pairwise_sqeuclidean(X, Z):\n",
    "    X2 = (X**2).sum(axis=1, keepdims=True)\n",
    "    Z2 = (Z**2).sum(axis=1, keepdims=True).T\n",
    "    return X2 + Z2 - 2 * (X @ Z.T)\n",
    "\n",
    "def _soft_assign(D, beta):\n",
    "    # Pc_{i j} ∝ exp(-β D_{i j})\n",
    "    D = D - D.min(axis=1, keepdims=True)       # stabilize\n",
    "    logits = -beta * D\n",
    "    logits -= logits.max(axis=1, keepdims=True)\n",
    "    P = np.exp(logits)\n",
    "    P /= P.sum(axis=1, keepdims=True) + 1e-12\n",
    "    return P\n",
    "\n",
    "def _normalize_Q(Pc, mu):\n",
    "    # Q_{i j} = mu_i * Pc_{i j} / sum_r mu_r * Pc_{r j}\n",
    "    num = mu[:, None] * Pc\n",
    "    den = num.sum(axis=0, keepdims=True) + 1e-12\n",
    "    return num / den\n",
    "\n",
    "def da_cluster_rows(W: torch.Tensor, M: int, beta=10.0, steps=200, tol=1e-6, seed=0):\n",
    "    \"\"\"\n",
    "    Cluster the ROWS of W (N x D) into M prototypes Z (M x D).\n",
    "    Returns: Pc (N x M), Z (M x D), info\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = W.detach().cpu().numpy()\n",
    "    N, D = X.shape\n",
    "    mu = np.ones(N) / N\n",
    "    idx = rng.choice(N, size=M, replace=False)\n",
    "    Z = X[idx].copy() + 1e-6 * rng.standard_normal((M, D))\n",
    "\n",
    "    prevF = np.inf\n",
    "    hist = []\n",
    "    for t in range(steps):\n",
    "        Dmat = _pairwise_sqeuclidean(X, Z)\n",
    "        Pc = _soft_assign(Dmat, beta)\n",
    "        Qc = _normalize_Q(Pc, mu)\n",
    "        Z = Qc.T @ X\n",
    "\n",
    "        # soft-min objective\n",
    "        Dmat = _pairwise_sqeuclidean(X, Z)\n",
    "        shifted = -beta * (Dmat - Dmat.min(axis=1, keepdims=True))\n",
    "        logsumexp = np.log(np.exp(shifted).sum(axis=1)) - beta * (-Dmat.min(axis=1))\n",
    "        F = - (mu * logsumexp).sum() / beta\n",
    "        hist.append(F)\n",
    "        if abs(prevF - F) < tol * (1 + abs(F)):\n",
    "            break\n",
    "        prevF = F\n",
    "\n",
    "    info = {\"iters\": t+1, \"F\": float(F), \"history_len\": len(hist)}\n",
    "    return Pc, Z, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def aggregate_layer(weights, sizes, hidden_layer_index, M, beta=10.0, seed=0):\n",
    "    \"\"\"\n",
    "    Compress hidden layer 'hidden_layer_index' (in terms of sizes).\n",
    "    weights[k] maps sizes[k] -> sizes[k+1].\n",
    "    We cluster rows of W_prev = weights[hidden_layer_index-1],\n",
    "    and right-multiply W_next = weights[hidden_layer_index] by Pc.\n",
    "    Returns: new_weights, new_sizes, metrics (with Pc/Z and pre-compression W_prev/W_next for block metrics).\n",
    "    \"\"\"\n",
    "    assert 0 < hidden_layer_index < len(sizes)-1, \"hidden_layer_index must refer to a hidden layer.\"\n",
    "\n",
    "    W_prev = weights[hidden_layer_index - 1]    # shape (old_width, in_dim)\n",
    "    W_next = weights[hidden_layer_index]        # shape (out_dim, old_width)\n",
    "    old_width = sizes[hidden_layer_index]\n",
    "    assert W_prev.shape[0] == old_width and W_next.shape[1] == old_width\n",
    "\n",
    "    Pc, Z, info = da_cluster_rows(W_prev, M=M, beta=beta, steps=200, seed=seed)  # Pc: (old_width, M), Z: (M, in_dim)\n",
    "\n",
    "    W_prev_new = torch.tensor(Z, dtype=W_prev.dtype)                 # (M, in_dim)\n",
    "    W_next_new = W_next @ torch.tensor(Pc, dtype=W_next.dtype)       # (out_dim, M)\n",
    "\n",
    "    # Reconstruction diagnostics\n",
    "    W_prev_approx = torch.tensor(Pc, dtype=W_prev.dtype) @ torch.tensor(Z, dtype=W_prev.dtype)\n",
    "    rel_err_prev = (frob(W_prev - W_prev_approx) / (frob(W_prev) + 1e-12)).item()\n",
    "    W_next_back = W_next_new @ torch.tensor(Pc, dtype=W_next.dtype).T\n",
    "    rel_err_next = (frob(W_next - W_next_back) / (frob(W_next) + 1e-12)).item()\n",
    "\n",
    "    # Splice into new weight list\n",
    "    new_weights = []\n",
    "    for k, W in enumerate(weights):\n",
    "        if k == hidden_layer_index - 1:\n",
    "            new_weights.append(W_prev_new)\n",
    "        elif k == hidden_layer_index:\n",
    "            new_weights.append(W_next_new)\n",
    "        else:\n",
    "            new_weights.append(W.clone())\n",
    "\n",
    "    new_sizes = sizes.copy()\n",
    "    new_sizes[hidden_layer_index] = M\n",
    "\n",
    "    metrics = {\n",
    "        \"layer_index\": int(hidden_layer_index),\n",
    "        \"old_width\": int(old_width),\n",
    "        \"new_width\": int(M),\n",
    "        \"rel_err_W_prev\": float(rel_err_prev),\n",
    "        \"rel_err_W_next\": float(rel_err_next),\n",
    "        \"da_info\": info,\n",
    "        # store what we need for local block comparisons\n",
    "        \"Pc\": Pc, \"Z\": Z,\n",
    "        \"W_prev_before\": W_prev.clone(),   # (old_width, in_dim)\n",
    "        \"W_next_before\": W_next.clone(),   # (out_dim, old_width)\n",
    "    }\n",
    "    return new_weights, new_sizes, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(7)\n",
    "sizes = [5, 10, 15, 10, 3]\n",
    "weights = init_mlp_weights(sizes, std=0.6)\n",
    "\n",
    "draw_network(weights, sizes, title=\"Original network (biases = 0)\")\n",
    "\n",
    "# Choose compressed widths\n",
    "M1, M2, M3 = 6, 7, 6\n",
    "beta = 10.0\n",
    "\n",
    "# Step 1: compress hidden layer 1 (size 10 -> M1)\n",
    "weights1, sizes1, m1 = aggregate_layer(weights, sizes, hidden_layer_index=1, M=M1, beta=beta, seed=11)\n",
    "\n",
    "# Step 2: compress hidden layer 2 (size 15 -> M2) on the updated network\n",
    "weights2, sizes2, m2 = aggregate_layer(weights1, sizes1, hidden_layer_index=2, M=M2, beta=beta, seed=12)\n",
    "\n",
    "# Step 3: compress hidden layer 3 (size 10 -> M3) on the updated network\n",
    "weights3, sizes3, m3 = aggregate_layer(weights2, sizes2, hidden_layer_index=3, M=M3, beta=beta, seed=13)\n",
    "\n",
    "draw_network(weights3, sizes3, title=f\"Coarse network: [10,15,10] → [{M1},{M2},{M3}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dissimilarity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1) End-to-end (linearized)\n",
    "A_orig   = end_to_end_map(weights)\n",
    "A_coarse = end_to_end_map(weights3)\n",
    "rel_err_global = (frob(A_orig - A_coarse) / (frob(A_orig) + 1e-12)).item()\n",
    "\n",
    "# 2) Local two-layer block errors around each compressed layer\n",
    "# For a hidden layer with W_prev (in→hidden) and W_next (hidden→out),\n",
    "# original composite:    B = W_next @ W_prev\n",
    "# compressed composite: B' = (W_next @ Pc) @ Z\n",
    "def block_rel_error(W_prev, W_next, Pc, Z):\n",
    "    B  = W_next @ W_prev\n",
    "    Bp = (W_next @ torch.tensor(Pc, dtype=W_next.dtype)) @ torch.tensor(Z, dtype=W_prev.dtype)\n",
    "    return (frob(B - Bp) / (frob(B) + 1e-12)).item()\n",
    "\n",
    "rel_err_B1 = block_rel_error(m1[\"W_prev_before\"], m1[\"W_next_before\"], m1[\"Pc\"], m1[\"Z\"])\n",
    "rel_err_B2 = block_rel_error(m2[\"W_prev_before\"], m2[\"W_next_before\"], m2[\"Pc\"], m2[\"Z\"])\n",
    "rel_err_B3 = block_rel_error(m3[\"W_prev_before\"], m3[\"W_next_before\"], m3[\"Pc\"], m3[\"Z\"])\n",
    "\n",
    "# 3) Per-layer recon errors already computed: rel_err_W_prev, rel_err_W_next\n",
    "\n",
    "print(\"Per-layer DA metrics:\")\n",
    "for tag, m in [(\"Step1\", m1), (\"Step2\", m2), (\"Step3\", m3)]:\n",
    "    print(f\" {tag} | layer {m['layer_index']}: {m['old_width']} → {m['new_width']}\"\n",
    "          f\" | recon(W_prev)={m['rel_err_W_prev']:.4f}\"\n",
    "          f\" | recon(W_next via Pc)={m['rel_err_W_next']:.4f}\"\n",
    "          f\" | DA iters={m['da_info']['iters']}\")\n",
    "\n",
    "print(\"\\nLocal block dissimilarities (two-layer composites around each compressed layer):\")\n",
    "print(f\" B1 (W2@W1 vs (W2@Pc1)@Z1): {rel_err_B1:.4f}\")\n",
    "print(f\" B2 (W3@W2 vs (W3@Pc2)@Z2): {rel_err_B2:.4f}\")\n",
    "print(f\" B3 (W4@W3 vs (W4@Pc3)@Z3): {rel_err_B3:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
